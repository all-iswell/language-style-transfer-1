{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a mind to strike thee ere thou speak’st.\n",
      "Yet if thou say Antony lives, is well, Or friends wi\n",
      "I have half a mind to hit you before you speak again.\n",
      "But if Antony is alive, healthy, friendly with\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "import os\n",
    "\n",
    "original_dir_path = '../data/shakespeare/original/'\n",
    "modern_dir_path = '../data/shakespeare/modern/'\n",
    "\n",
    "def read_all_files(dir_path):\n",
    "    docs = \"\"\n",
    "    for filename in os.listdir(dir_path):\n",
    "        with open(dir_path + filename, 'r') as file:\n",
    "            docs += file.read()\n",
    "    return docs\n",
    "\n",
    "original_docs = read_all_files(original_dir_path)\n",
    "modern_docs = read_all_files(modern_dir_path)\n",
    "\n",
    "print(original_docs[:100])\n",
    "print(modern_docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'mind', 'to', 'strike', 'thee', 'ere', 'thou', 'speak’st', 'Yet', 'if', 'thou', 'say', 'Antony', 'lives,', 'is', 'well,', 'Or', 'friends', 'with', 'Caesar,', 'or', 'not', 'captive', 'to', 'him,', 'I’ll', 'set', 'thee', 'in', 'a', 'shower', 'of', 'gold', 'and', 'hail', 'Rich', 'pearls', 'upon', 'thee', 'Madam,', 'he’s', 'well', 'Well', 'said', 'And', 'friends', 'with', 'Caesar', 'Th’', 'art', 'an', 'honest', 'man', 'Caesar', 'and', 'he', 'are', 'greater', 'friends', 'than', 'ever', 'Make', 'thee', 'a', 'fortune', 'from', 'me', 'But', 'yet,', 'madam—', 'I', 'do', 'not', 'like', '“But', 'yet”', 'It', 'does', 'allay', 'The', 'good', 'precedence', 'Fie', 'upon', '“But', 'yet”', '“But', 'yet”', 'is', 'as', 'a', 'jailer', 'to', 'bring', 'forth', 'Some', 'monstrous', 'malefactor']\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode entire dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = (original_docs + \" \" + modern_docs).replace(\".\",\"\").replace(\"\\n\", \" \").split(\" \")\n",
    "print(dataset[:100])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(dataset)\n",
    "V = len(enc.classes_) #size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "original_sentences = original_docs.replace('.',\"\").split('\\n')\n",
    "modern_sentences = modern_docs.replace('.',\"\").split('\\n')\n",
    "\n",
    "original_sentences = original_sentences\n",
    "modern_sentences = modern_sentences\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "X_org = []\n",
    "for sentence in original_sentences:\n",
    "    words = sentence.split(\" \")\n",
    "    try:\n",
    "        words = words[:30]\n",
    "    except:\n",
    "        pass\n",
    "    word_idx = np.array(enc.transform(words))\n",
    "    \n",
    "    arr = np.zeros(max_length)\n",
    "    arr[:len(words)] = word_idx\n",
    "    X_org.append(arr)\n",
    "    \n",
    "X_modern = []\n",
    "for sentence in modern_sentences:\n",
    "    words = sentence.split(\" \")\n",
    "    try:\n",
    "        words = words[:30]\n",
    "    except:\n",
    "        pass\n",
    "    word_idx = np.array(enc.transform(words))\n",
    "    \n",
    "    arr = np.zeros(max_length)\n",
    "    arr[:len(words)] = word_idx\n",
    "    X_modern.append(arr)\n",
    "\n",
    "X_org = np.array(X_org)\n",
    "X_mod = np.array(X_modern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "X_dict = {'X_org': X_org, 'X_mod': X_mod}\n",
    "pickle_path = '../models/X_shakespeare_ohe.pickle'\n",
    "\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(X_dict, f)\n",
    "\n",
    "X_dict_loaded = None\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    X_dict_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overfit Autoencoder\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def dense(x, n1, n2, name):\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        weights = tf.get_variable(\"weights\", shape=[n1, n2], initializer=tf.random_normal_initializer(mean=0, stddev=0.01))\n",
    "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out\n",
    "\n",
    "input_dim = max_length\n",
    "n_l1 = 100\n",
    "n_l2 = 100\n",
    "z_dim = 2\n",
    "\n",
    "def encoder(x, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Encoder'):\n",
    "        e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n",
    "        e_dense_2 = tf.nn.relu(dense(e_dense_1, n_l1, n_l2, 'e_dense_2'))\n",
    "        latent_variable = dense(e_dense_2, n_l2, z_dim, 'e_latent_variable')\n",
    "        return latent_variable\n",
    "    \n",
    "def decoder(z, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_varaiable_scope().reuse_variables()\n",
    "    with tf.name_scope('Decodr'):\n",
    "        d_dense_1 = tf.nn.relu(dense(z, z_dim, n_l2, 'd_dense_1'))\n",
    "        d_dense_2 = tf.nn.relu(dense(d_dense_1, n_l2, n_l1, 'd_dense_2'))\n",
    "        output = dense(d_dense_2, n_l2, input_dim, 'd_output')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, [None, max_length])\n",
    "\n",
    "encoder_output = encoder(x_input)\n",
    "decoder_output = decoder(encoder_output)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(decoder_output - x_input))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10412233.0\n",
      "Loss: 4148458.5\n",
      "Loss: 4148045.25\n",
      "Loss: 4146912.75\n",
      "Loss: 4145893.5\n",
      "Loss: 4147863.5\n",
      "Loss: 4146901.0\n",
      "Loss: 4146691.0\n",
      "Loss: 4146446.25\n",
      "Loss: 4146140.25\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 10\n",
    "X = X_org\n",
    "batch_size = 100\n",
    "\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[:(b + 1) * batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x})\n",
    "            if i % 100 == 0:\n",
    "                batch_loss = sess.run(loss, feed_dict={x_input: batch_x})\n",
    "                print(\"Loss: {0}\".format(batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_encoder(x, lstm_units=2, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope('Encoder'):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units, initializer=initializer)\n",
    "\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_fw, x, dtype=tf.float32)\n",
    "        return state\n",
    "\n",
    "def lstm_decoder(x, z, lstm_units=2, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope('Decoder'):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units, initializer=initializer)\n",
    "        \n",
    "        zero_tensor = tf.zeros_like(x)\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_fw, zero_tensor, initial_state=z)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 30, 29309)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "\n",
    "x_input = tf.placeholder(tf.int32, [None, 30])\n",
    "#embedding = tf.expand_dims(x_input, axis=2)\n",
    "embedding = tf.one_hot(x_input, V)\n",
    "\n",
    "encoder_output = lstm_encoder(embedding, lstm_units=V)\n",
    "decoder_output = lstm_decoder(embedding, encoder_output, lstm_units=V)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(decoder_output - embedding))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 1000\n",
    "X = X_org\n",
    "batch_size = 100\n",
    "\n",
    "losses = []\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[:(b + 1) * batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x})\n",
    "            \n",
    "            if b % 10 == 0:\n",
    "                loss_i = sess.run(loss, feed_dict={x_input: X})\n",
    "                print(\"Loss: {0}\".format(loss_i))\n",
    "                losses.append(loss_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-Level LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a mind to strike thee ere thou speak’st.\n",
      "Yet if thou say Antony lives, is well, Or friends wi\n",
      "I have half a mind to hit you before you speak again.\n",
      "But if Antony is alive, healthy, friendly with\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "import os\n",
    "\n",
    "original_dir_path = '../data/shakespeare/original/'\n",
    "modern_dir_path = '../data/shakespeare/modern/'\n",
    "\n",
    "def read_all_files(dir_path):\n",
    "    docs = \"\"\n",
    "    for filename in os.listdir(dir_path):\n",
    "        with open(dir_path + filename, 'r') as file:\n",
    "            docs += file.read()\n",
    "    return docs\n",
    "\n",
    "original_docs = read_all_files(original_dir_path)\n",
    "modern_docs = read_all_files(modern_dir_path)\n",
    "\n",
    "print(original_docs[:100])\n",
    "print(modern_docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'm', 'i', 'n', 'd', ' ', 't', 'o', ' ', 's', 't', 'r', 'i', 'k', 'e', ' ', 't', 'h', 'e', 'e', ' ', 'e', 'r', 'e', ' ', 't', 'h', 'o', 'u', ' ', 's', 'p', 'e', 'a', 'k', '’', 's', 't', ' ', 'Y', 'e', 't', ' ', 'i', 'f', ' ', 't', 'h', 'o', 'u', ' ', 's', 'a', 'y', ' ', 'A', 'n', 't', 'o', 'n', 'y', ' ', 'l', 'i', 'v', 'e', 's', ',', ' ', 'i', 's', ' ', 'w', 'e', 'l', 'l', ',', ' ', 'O', 'r', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ' ', 'w', 'i', 't']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode entire dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = list((original_docs + \" \" + modern_docs).replace(\".\",\"\").replace(\"\\n\", \" \"))\n",
    "print(dataset[:100])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(dataset)\n",
    "V = len(enc.classes_) #size of vocabulary\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinpark/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(200, -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matrify_sentences(sentences, encoder, max_length=200):\n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        letters = list(sentence)\n",
    "        try:\n",
    "            letters = letters[:max_length]\n",
    "        except:\n",
    "            pass\n",
    "        letters_idx = np.array(encoder.transform(letters))\n",
    "        \n",
    "        arr = np.full(max_length, -1)\n",
    "        arr[:len(letters)] = letters_idx\n",
    "        X.append(arr)\n",
    "    \n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "original_sentences = original_docs.replace('.',\"\").split('\\n')\n",
    "modern_sentences = modern_docs.replace('.',\"\").split('\\n')\n",
    "\n",
    "X_org = matrify_sentences(original_sentences, enc)\n",
    "X_mod = matrify_sentences(modern_sentences, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "max_length = 20\n",
    "\n",
    "x_input = tf.placeholder(tf.int32, [None, max_length])\n",
    "#embedding = tf.expand_dims(x_input, axis=2)\n",
    "embedding = tf.one_hot(x_input, V)\n",
    "\n",
    "encoder_output = lstm_encoder(embedding, lstm_units=V)\n",
    "decoder_output = lstm_decoder(embedding, encoder_output, lstm_units=V)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(decoder_output - embedding))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 1000\n",
    "X = X_org[:,:max_length]\n",
    "batch_size = 100\n",
    "saved_model_path = '../models/lstm_ae.ckpt'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "losses = []\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[:(b + 1) * batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x})\n",
    "            \n",
    "            if b % 50 == 0:\n",
    "                loss_i = sess.run(loss, feed_dict={x_input: batch_x})\n",
    "                print(\"Loss: {0}\".format(loss_i))\n",
    "                losses.append(loss_i)\n",
    "                \n",
    "                saver.save(sess, saved_model_path, global_step=step)\n",
    "            step += batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
