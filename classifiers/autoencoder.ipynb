{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a mind to strike thee ere thou speak’st.\n",
      "Yet if thou say Antony lives, is well, Or friends wi\n",
      "I have half a mind to hit you before you speak again.\n",
      "But if Antony is alive, healthy, friendly with\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "import os\n",
    "\n",
    "original_dir_path = '../data/shakespeare/original/'\n",
    "modern_dir_path = '../data/shakespeare/modern/'\n",
    "\n",
    "def read_all_files(dir_path):\n",
    "    docs = \"\"\n",
    "    for filename in os.listdir(dir_path):\n",
    "        with open(dir_path + filename, 'r') as file:\n",
    "            docs += file.read()\n",
    "    return docs\n",
    "\n",
    "original_docs = read_all_files(original_dir_path)\n",
    "modern_docs = read_all_files(modern_dir_path)\n",
    "\n",
    "print(original_docs[:100])\n",
    "print(modern_docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'mind', 'to', 'strike', 'thee', 'ere', 'thou', 'speak’st', 'Yet', 'if', 'thou', 'say', 'Antony', 'lives,', 'is', 'well,', 'Or', 'friends', 'with', 'Caesar,', 'or', 'not', 'captive', 'to', 'him,', 'I’ll', 'set', 'thee', 'in', 'a', 'shower', 'of', 'gold', 'and', 'hail', 'Rich', 'pearls', 'upon', 'thee', 'Madam,', 'he’s', 'well', 'Well', 'said', 'And', 'friends', 'with', 'Caesar', 'Th’', 'art', 'an', 'honest', 'man', 'Caesar', 'and', 'he', 'are', 'greater', 'friends', 'than', 'ever', 'Make', 'thee', 'a', 'fortune', 'from', 'me', 'But', 'yet,', 'madam—', 'I', 'do', 'not', 'like', '“But', 'yet”', 'It', 'does', 'allay', 'The', 'good', 'precedence', 'Fie', 'upon', '“But', 'yet”', '“But', 'yet”', 'is', 'as', 'a', 'jailer', 'to', 'bring', 'forth', 'Some', 'monstrous', 'malefactor']\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode entire dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = (original_docs + \" \" + modern_docs).replace(\".\",\"\").replace(\"\\n\", \" \").split(\" \")\n",
    "print(dataset[:100])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(dataset)\n",
    "V = len(enc.classes_) #size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "original_sentences = original_docs.replace('.',\"\").split('\\n')\n",
    "modern_sentences = modern_docs.replace('.',\"\").split('\\n')\n",
    "\n",
    "original_sentences = original_sentences\n",
    "modern_sentences = modern_sentences\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "X_org = []\n",
    "for sentence in original_sentences:\n",
    "    words = sentence.split(\" \")\n",
    "    try:\n",
    "        words = words[:30]\n",
    "    except:\n",
    "        pass\n",
    "    word_idx = np.array(enc.transform(words))\n",
    "    \n",
    "    arr = np.zeros(max_length)\n",
    "    arr[:len(words)] = word_idx\n",
    "    X_org.append(arr)\n",
    "    \n",
    "X_modern = []\n",
    "for sentence in modern_sentences:\n",
    "    words = sentence.split(\" \")\n",
    "    try:\n",
    "        words = words[:30]\n",
    "    except:\n",
    "        pass\n",
    "    word_idx = np.array(enc.transform(words))\n",
    "    \n",
    "    arr = np.zeros(max_length)\n",
    "    arr[:len(words)] = word_idx\n",
    "    X_modern.append(arr)\n",
    "\n",
    "X_org = np.array(X_org)\n",
    "X_mod = np.array(X_modern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "X_dict = {'X_org': X_org, 'X_mod': X_mod}\n",
    "pickle_path = '../models/X_shakespeare_ohe.pickle'\n",
    "\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(X_dict, f)\n",
    "\n",
    "X_dict_loaded = None\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    X_dict_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overfit Autoencoder\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def dense(x, n1, n2, name):\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        weights = tf.get_variable(\"weights\", shape=[n1, n2], initializer=tf.random_normal_initializer(mean=0, stddev=0.01))\n",
    "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out\n",
    "\n",
    "input_dim = max_length\n",
    "n_l1 = 100\n",
    "n_l2 = 100\n",
    "z_dim = 2\n",
    "\n",
    "def encoder(x, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Encoder'):\n",
    "        e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n",
    "        e_dense_2 = tf.nn.relu(dense(e_dense_1, n_l1, n_l2, 'e_dense_2'))\n",
    "        latent_variable = dense(e_dense_2, n_l2, z_dim, 'e_latent_variable')\n",
    "        return latent_variable\n",
    "    \n",
    "def decoder(z, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_varaiable_scope().reuse_variables()\n",
    "    with tf.name_scope('Decodr'):\n",
    "        d_dense_1 = tf.nn.relu(dense(z, z_dim, n_l2, 'd_dense_1'))\n",
    "        d_dense_2 = tf.nn.relu(dense(d_dense_1, n_l2, n_l1, 'd_dense_2'))\n",
    "        output = dense(d_dense_2, n_l2, input_dim, 'd_output')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, [None, max_length])\n",
    "\n",
    "encoder_output = encoder(x_input)\n",
    "decoder_output = decoder(encoder_output)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(decoder_output - x_input))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10412233.0\n",
      "Loss: 4148458.5\n",
      "Loss: 4148045.25\n",
      "Loss: 4146912.75\n",
      "Loss: 4145893.5\n",
      "Loss: 4147863.5\n",
      "Loss: 4146901.0\n",
      "Loss: 4146691.0\n",
      "Loss: 4146446.25\n",
      "Loss: 4146140.25\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 10\n",
    "X = X_org\n",
    "batch_size = 100\n",
    "\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[:(b + 1) * batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x})\n",
    "            if i % 100 == 0:\n",
    "                batch_loss = sess.run(loss, feed_dict={x_input: batch_x})\n",
    "                print(\"Loss: {0}\".format(batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_encoder(x, lstm_units=2, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope('Encoder'):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units, initializer=initializer)\n",
    "\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_fw, x, dtype=tf.float32)\n",
    "        return state\n",
    "\n",
    "def lstm_decoder(x, z, lstm_units=2, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope('Decoder'):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units, initializer=initializer)\n",
    "        \n",
    "        zero_tensor = tf.zeros_like(x)\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_fw, zero_tensor, initial_state=z)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 30, 29309)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "\n",
    "x_input = tf.placeholder(tf.int32, [None, 30])\n",
    "#embedding = tf.expand_dims(x_input, axis=2)\n",
    "embedding = tf.one_hot(x_input, V)\n",
    "\n",
    "encoder_output = lstm_encoder(embedding, lstm_units=V)\n",
    "decoder_output = lstm_decoder(embedding, encoder_output, lstm_units=V)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(decoder_output - embedding))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 1000\n",
    "X = X_org\n",
    "batch_size = 100\n",
    "\n",
    "losses = []\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[:(b + 1) * batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x})\n",
    "            \n",
    "            if b % 10 == 0:\n",
    "                loss_i = sess.run(loss, feed_dict={x_input: X})\n",
    "                print(\"Loss: {0}\".format(loss_i))\n",
    "                losses.append(loss_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-Level LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a mind to strike thee ere thou speak’st.\n",
      "Yet if thou say Antony lives, is well, Or friends wi\n",
      "I have half a mind to hit you before you speak again.\n",
      "But if Antony is alive, healthy, friendly with\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "import os\n",
    "\n",
    "original_dir_path = '../data/shakespeare/original/'\n",
    "modern_dir_path = '../data/shakespeare/modern/'\n",
    "\n",
    "def read_all_files(dir_path):\n",
    "    docs = \"\"\n",
    "    for filename in os.listdir(dir_path):\n",
    "        with open(dir_path + filename, 'r') as file:\n",
    "            docs += file.read()\n",
    "    return docs\n",
    "\n",
    "original_docs = read_all_files(original_dir_path)\n",
    "modern_docs = read_all_files(modern_dir_path)\n",
    "\n",
    "print(original_docs[:100])\n",
    "print(modern_docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'm', 'i', 'n', 'd', ' ', 't', 'o', ' ', 's', 't', 'r', 'i', 'k', 'e', ' ', 't', 'h', 'e', 'e', ' ', 'e', 'r', 'e', ' ', 't', 'h', 'o', 'u', ' ', 's', 'p', 'e', 'a', 'k', '’', 's', 't', ' ', 'Y', 'e', 't', ' ', 'i', 'f', ' ', 't', 'h', 'o', 'u', ' ', 's', 'a', 'y', ' ', 'A', 'n', 't', 'o', 'n', 'y', ' ', 'l', 'i', 'v', 'e', 's', ',', ' ', 'i', 's', ' ', 'w', 'e', 'l', 'l', ',', ' ', 'O', 'r', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ' ', 'w', 'i', 't']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode entire dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = list((original_docs + \" \" + modern_docs).replace(\".\",\"\").replace(\"\\n\", \" \"))\n",
    "print(dataset[:100])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(dataset)\n",
    "V = len(enc.classes_) #size of vocabulary\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinpark/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(200, -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matrify_sentences(sentences, encoder, max_length=200):\n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        letters = list(sentence)\n",
    "        try:\n",
    "            letters = letters[:max_length]\n",
    "        except:\n",
    "            pass\n",
    "        letters_idx = np.array(encoder.transform(letters))\n",
    "        \n",
    "        arr = np.full(max_length, -1)\n",
    "        arr[:len(letters)] = letters_idx\n",
    "        X.append(arr)\n",
    "    \n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "original_sentences = original_docs.replace('.',\"\").split('\\n')\n",
    "modern_sentences = modern_docs.replace('.',\"\").split('\\n')\n",
    "\n",
    "X_org = matrify_sentences(original_sentences, enc)\n",
    "X_mod = matrify_sentences(modern_sentences, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "max_length = 20\n",
    "\n",
    "x_input = tf.placeholder(tf.int32, [None, max_length])\n",
    "#embedding = tf.expand_dims(x_input, axis=2)\n",
    "embedding = tf.one_hot(x_input, V)\n",
    "\n",
    "encoder_output = lstm_encoder(embedding, lstm_units=V)\n",
    "decoder_output = lstm_decoder(embedding, encoder_output, lstm_units=V)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(decoder_output - embedding))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/lstm_ae.ckpt-47000\n",
      "Loss: 0.00767822191119194\n",
      "Loss: 0.00681416317820549\n",
      "Loss: 0.006161660887300968\n",
      "Loss: 0.005839089397341013\n",
      "Loss: 0.005440966226160526\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 20\n",
    "X = X_org[:,:max_length]\n",
    "batch_size = 100\n",
    "saved_model_path = '../models/lstm_ae/lstm_ae.ckpt'\n",
    "checkpoint_dir = '../models/lstm_ae/'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "losses = []\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    except:\n",
    "        print('No model')\n",
    "        sess.run(init)\n",
    "        \n",
    "    # Train\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[b * batch_size:(b + 1) * batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x})\n",
    "            \n",
    "            if i % 2 == 0 and b == 0:\n",
    "                loss_i = sess.run(loss, feed_dict={x_input: batch_x})\n",
    "                print(\"Loss: {0}\".format(loss_i))\n",
    "                losses.append(loss_i)\n",
    "\n",
    "                saver.save(sess, saved_model_path, global_step=step)\n",
    "            step += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/lstm_ae_wa/lstm_ae_wa.ckpt-0\n",
      "['What will you do?', 'It was some fiend', 'Where’s the king?', 'Here, sir, as fooli', 'He that plays the k']\n",
      "['dddddddnnnnnnnnnnnn', ',,,,,,,,nnnnnnnnnnn', 'rrVVVVVVVVVViiiiiii', '’’’’’’’’\\xa0\\xa0\\xa0iiiiiiii', 'RRRRRbbbbbbbbbbbbdd']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinpark/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(5, -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# test AE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "samples = X[np.random.choice(X.shape[0], size=5)]\n",
    "\n",
    "def textify_samples(x):\n",
    "    x[:,-1] = np.full(x.shape[0], -1)\n",
    "    eos_idx = np.argmin(x, axis=1)\n",
    "    x = [x[i,:eos_idx[i]] for i in range(x.shape[0])]\n",
    "\n",
    "    x_text = [enc.inverse_transform(x[i].astype(int)) for i in range(len(x))]\n",
    "    x_text = [''.join(list(text)) for text in x_text]\n",
    "    return x_text\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    x_generated = sess.run(decoder_output, feed_dict={x_input: samples})\n",
    "    x_generated = np.argmax(x_generated, axis=2)\n",
    "    \n",
    "    print(textify_samples(samples))\n",
    "    print(textify_samples(x_generated))\n",
    "#sess.run(decoder_output, feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder with adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Compute the leaky ReLU activation function.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor with arbitrary shape\n",
    "    - alpha: leak parameter for leaky ReLU\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with the same shape as x\n",
    "    \"\"\"\n",
    "    # TODO: implement leaky ReLU\n",
    "    out = tf.maximum(tf.cast(0.0, dtype='float64'), tf.cast(x, dtype='float64'))\n",
    "    out1 = tf.minimum(tf.cast(0.0, dtype='float64'), tf.cast(alpha * x, dtype='float64'))\n",
    "    return tf.cast(out + out1, dtype='float32')\n",
    "\n",
    "def discriminator(x, reuse=False):\n",
    "    \"\"\"Compute discriminator score for a batch of input images.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor of flattened input images, shape [batch_size, 784]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with shape [batch_size, 1], containing the score \n",
    "    for an image being real for each input image.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        # TODO: implement architecture\n",
    "        x = tf.layers.dense(x, 10)\n",
    "        x = leaky_relu(x)\n",
    "        x = tf.layers.dense(x, 10)\n",
    "        x = leaky_relu(x)\n",
    "        x = tf.layers.dense(x, 1)\n",
    "        logits = x\n",
    "        return logits\n",
    "    \n",
    "def lstm_discriminator(x, lstm_units=2, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope('discriminator'):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units, initializer=initializer)\n",
    "\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_fw, x, dtype=tf.float32)\n",
    "        logits = tf.layers.dense(outputs, 1)\n",
    "        return logits\n",
    "    \n",
    "def gan_loss(logits_real, logits_fake):\n",
    "    \"\"\"Compute the GAN loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - logits_real: Tensor, shape [batch_size, 1], output of discriminator\n",
    "        Log probability that the image is real for each real image\n",
    "    - logits_fake: Tensor, shape[batch_size, 1], output of discriminator\n",
    "        Log probability that the image is real for each fake image\n",
    "    \n",
    "    Returns:\n",
    "    - D_loss: discriminator loss scalar\n",
    "    - G_loss: generator loss scalar\n",
    "    \"\"\"\n",
    "    # TODO: compute D_loss and G_loss\n",
    "    labels_real = tf.ones_like(logits_real)\n",
    "    labels_fake = tf.zeros_like(logits_fake)\n",
    "    logits_fake_inv = -logits_fake\n",
    "    \n",
    "    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_real, logits=logits_real) +\n",
    "                            tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_fake, logits=logits_fake))\n",
    "    \n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_real, logits=logits_fake))\n",
    "\n",
    "    return D_loss, G_loss\n",
    "\n",
    "def get_solvers(learning_rate=1e-3, beta1=0.5):\n",
    "    \"\"\"Create solvers for GAN training.\n",
    "    \n",
    "    Inputs:\n",
    "    - learning_rate: learning rate to use for both solvers\n",
    "    - beta1: beta1 parameter for both solvers (first moment decay)\n",
    "    \n",
    "    Returns:\n",
    "    - D_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    - G_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    \"\"\"\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    G_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    return D_solver, G_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "max_length = 20\n",
    "\n",
    "x_input = tf.placeholder(tf.int32, [None, None])\n",
    "#embedding = tf.expand_dims(x_input, axis=2)\n",
    "embedding = tf.one_hot(x_input, V)\n",
    "\n",
    "encoder_output = lstm_encoder(embedding, lstm_units=V)\n",
    "decoder_output = lstm_decoder(embedding, encoder_output, lstm_units=V)\n",
    "\n",
    "with tf.variable_scope(\"\") as scope:\n",
    "    logits_real = lstm_discriminator(embedding)\n",
    "    logits_fake = lstm_discriminator(decoder_output, reuse=True) # might need to predict instead (argmax)\n",
    "\n",
    "D_solver, G_solver = get_solvers()\n",
    "D_loss, G_loss = gan_loss(logits_real, logits_fake)\n",
    "\n",
    "D_train_step = D_solver.minimize(D_loss)\n",
    "G_train_step = G_solver.minimize(G_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 1.387786626815796, G_loss: 0.6922908425331116\n",
      "0\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['ZZ', 'èè', 'cc', 'tt', '––', 'rr', 'OO', 'ZT', 'è1', '??', 'ZW', 'Mt', 'èè', 'VZ', 'Mt', 'bb', 'bb', 'YY', 'cc', 'OO', 'oo', 'OO', 'j–', '\\xa0\\xa0', \"''\", 'LL', 'OO', 'Mt', '\\u2003c', 'LL', 'OO', 'ZZ', 'oo', 'OO', 'tt', 'tt', 'bO', \"''\", 'bO', 'MM', '–:', 'OO', 'tt', \"''\", 'Vp', 'ZZ', 'RZ', 'OO', 'KK', 'bb', 'DD', '\\xa0\\xa0', 'MM', 'Zo', 'jj', 'Mt', 'lZ', 'èè', 'OO', 'xx', 'oo', 'UU', 'aa', 'YY', 'Mx', 'Pj', '?W', 'çç', 'èè', ';;', 'Mx', 'bb', 'YZ', 'VZ', 'bb', 'gg', 'IU', 'CC', '\\xa0t', 'lU', 'jh', \"t'\", 'OO', 'Zo', \"''\", 'tt', 'ZZ', 'ZZ', '\\xa0t', 'OO', '’L', 'jh', '––', 'LL', 'OO', 'oo', 'èè', 'çç', 'OO', 'tt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinpark/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(100, -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss: 0.7650749683380127, G_loss: 0.801673948764801\n",
      "4\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['VV', 'Sx', 'ææ', 'Mt', 'Rl', 'rr', 'OO', 'TT', 'OO', '\\xa0x', ';m', 'Mæ', 'Sb', 'YV', 'Mæ', 'bb', 'bb', 'YY', 'ææ', 'OO', 'oo', 'OO', '““', 'N\\xa0', 'oo', 'L,', 'OO', 'Mæ', 'DD', 'ww', 'OO', 'VV', 'oo', 'OO', 'bK', '“Æ', 'bb', 'oo', 'bb', 'MM', 'HH', 'OO', '  ', 'oo', 'V5', 'VV', 'RR', 'OO', \"K'\", 'bb', 'DD', '\\xa0\\xa0', 'MM', 'nn', 'R(', 'Mæ', 'lY', '…5', 'OO', 'sx', 'oo', 'TT', 'TT', 'YY', 'Mx', 'PP', 'WW', ';;', 'H5', ';;', 'Mx', 'bb', 'YY', 'YV', 'bb', '((', 'IE', 'Æx', '\\xa0\\xa0', 'nn', ',-', \"''\", 'OO', 'nn', 'oo', 'ït', 'PP', 'PP', '\\xa0\\xa0', 'OO', 'LL', ',-', 'Rl', ',,', 'OO', 'ob', 'H5', 'çç', 'OO', 'Mt']\n",
      "D_loss: 0.7443634271621704, G_loss: 0.7777029275894165\n",
      "8\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['VV', 'xx', 'ææ', 'Mx', 'Rl', 'rr', 'xx', 'T(', ',x', 'xx', ';(', 'ææ', 'bx', 'ææ', 'ææ', 'bb', 'bb', '\\xa0(', 'ææ', 'Ox', \"'x\", 'OO', '“x', '\\xa0\\xa0', 'o(', 'Lx', 'Ox', 'ææ', 'Dx', 'wx', 'OO', 'VV', \"'x\", 'Ox', 'bt', '\\xa0x', 'QQ', 'o(', 'QQ', 'MM', 'æ(', 'Ox', 'vx', 'o(', 'p5', 'VV', 'RR', 'Ox', 'Kx', 'bb', 'A(', '\\xa0\\xa0', 'MM', 'n(', '((', 'ææ', 'l(', '\\xa0x', 'Ox', 'xx', \"'x\", 'T(', 'T(', '\\xa0(', 'xx', 'P\\xa0', '…x', ';;', 'xx', 'E;', 'xx', 'bb', 'Y(', 'ææ', 'b(', '((', 'I(', 'xx', '\\xa0x', 'nn', ',x', \"'x\", 'OO', 'n(', 'o(', 'ït', 'Pæ', 'Pæ', '\\xa0x', 'Ox', '’x', ',x', 'Rl', ',x', 'Ox', 'o(', 'xx', 'Vç', 'Ox', 'Mx']\n",
      "D_loss: 0.7583627700805664, G_loss: 0.763138473033905\n",
      "12\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['\\xa0x', 'xx', 'æx', 'xx', 'lx', 'rx', 'xx', 'Tx', 'xx', 'xx', 'xx', 'æx', 'xx', 'æx', 'æx', 'bx', 'bx', '\\xa0x', 'æx', 'xx', 'xx', 'Ox', 'xx', '\\xa0x', 'xx', 'xx', 'xx', 'æx', 'xx', 'xx', 'Ox', '\\xa0x', 'xx', 'xx', 'bx', 'xx', 'bx', 'xx', 'bx', 'Mx', 'æx', 'xx', 'xx', 'xx', 'px', '\\xa0x', 'px', 'xx', 'xx', 'bx', 'Ax', '\\xa0x', 'Mx', 'nx', 'xx', 'æx', 'lx', 'xx', 'xx', 'xx', 'xx', 'Tx', 'xx', '\\xa0x', 'xx', '\\xa0x', 'xx', ';x', 'xx', 'Ex', 'xx', 'bx', 'xx', 'æx', 'bx', '(x', 'xx', 'xx', 'xx', 'nx', 'xx', 'xx', 'Ox', 'nx', 'xx', 'ïx', 'xx', 'xx', 'xx', 'xx', 'xx', 'xx', 'lx', 'xx', 'xx', 'xx', 'xx', 'çx', 'xx', 'xx']\n",
      "D_loss: 0.761307418346405, G_loss: 0.7593516707420349\n",
      "16\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['xx', 'xx', 'æx', 'xx', 'lx', 'rx', 'xx', 'Tx', 'xx', 'xx', 'xx', 'æx', 'xx', 'æx', 'æx', 'bx', 'bx', 'xx', 'æx', 'xx', 'xx', 'xx', 'xx', '\\xa0x', 'xx', 'xx', 'xx', 'æx', 'xx', 'xx', 'xx', 'xx', 'xx', 'xx', 'xx', 'xx', 'bx', 'xx', 'bx', 'xx', 'æx', 'xx', 'xx', 'xx', 'xx', 'xx', 'px', 'xx', 'xx', 'bx', 'Ax', '\\xa0x', 'xx', 'nx', 'xx', 'æx', 'lx', 'xx', 'xx', 'xx', 'xx', 'Tx', 'xx', 'xx', 'xx', '\\xa0x', 'xx', ';x', 'xx', 'xx', 'xx', 'bx', 'xx', 'æx', 'bx', 'xx', 'xx', 'xx', 'xx', 'nx', 'xx', 'xx', 'xx', 'nx', 'xx', 'xx', 'xx', 'xx', 'xx', 'xx', 'xx', 'xx', 'lx', 'xx', 'xx', 'xx', 'xx', 'çx', 'xx', 'xx']\n",
      "D_loss: 0.7606702446937561, G_loss: 0.7571764588356018\n",
      "20\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['px', 'xx', 'æx', 'xx', 'lx', 'rx', 'xx', 'Tx', 'px', 'xx', 'xx', '\\xa0x', 'xx', 'æx', '\\xa0x', 'bx', 'bx', '\\xa0x', 'æx', 'xx', 'xx', 'xx', 'xx', '\\xa0x', 'xx', 'xx', 'xx', '\\xa0x', 'xx', 'xx', 'xx', 'px', 'xx', 'xx', 'xx', 'xx', 'bx', 'xx', 'bx', 'Mx', 'æp', 'xx', ' x', 'xx', 'px', 'px', 'px', 'xx', 'xx', 'bx', 'Ax', '\\xa0p', 'Mx', 'nx', 'xx', '\\xa0x', 'lx', 'xx', 'xx', 'xx', 'xx', 'Tx', 'xx', '\\xa0x', 'xx', '\\xa0x', 'xx', ';x', 'xx', ';x', 'xx', 'bx', 'px', 'æx', 'bx', 'xx', 'xx', 'xx', 'px', 'pp', 'px', 'xx', 'xx', 'nx', 'xx', 'tx', 'xx', 'xx', 'px', 'xx', 'xx', 'px', 'lx', 'xx', 'xx', 'xx', 'xx', 'çx', 'xx', 'xx']\n",
      "D_loss: 0.7579908967018127, G_loss: 0.7568719387054443\n",
      "24\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['pp', 'xx', 'æp', 'xx', '“x', 'pp', 'xx', 'Tx', 'pp', 'xx', 'xx', 'pp', 'xx', 'æp', 'pp', 'bp', 'bp', '\\xa0x', 'æp', 'xx', 'xx', 'xx', 'xx', '\\xa0p', 'xx', 'xx', 'xx', 'pp', 'pp', 'xx', 'xx', 'pp', 'xx', 'xx', 'xx', 'xx', ' x', 'xx', ' x', 'pp', 'æp', 'xx', ' x', 'xx', 'pp', 'pp', 'pp', 'xx', 'pp', 'bp', 'Ap', 'pp', 'pp', 'nx', 'pp', 'pp', 'pp', 'xx', 'xx', 'xx', 'xx', 'pp', 'xx', '\\xa0x', 'xx', '\\xa0x', 'xx', ';p', 'xx', 'pp', 'xx', 'bx', 'pp', 'æp', 'bx', 'pp', 'xx', 'xx', 'pp', 'pp', 'pp', 'pp', 'xx', 'nx', 'xx', 'tp', 'xx', 'xx', 'pp', 'xx', 'xx', 'pp', '“x', 'xx', 'xx', 'xx', 'xx', 'Jp', 'xx', 'xx']\n",
      "D_loss: 0.7627776861190796, G_loss: 0.7494302988052368\n",
      "28\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['pp', 'xp', 'æp', 'xp', '“p', 'pp', 'xx', 'Tp', 'pp', 'xx', 'pp', 'pp', 'xp', 'pp', 'pp', 'bp', 'bp', 'pp', 'æp', 'xp', 'pp', 'pp', 'pp', '\\xa0p', 'xp', 'xp', 'xp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'xp', 'bp', 'xp', ' p', 'xp', ' p', 'pp', 'pp', 'xp', ' p', 'xp', 'pp', 'pp', 'pp', 'xp', 'pp', 'bp', 'pp', 'pp', 'pp', 'np', 'pp', 'pp', 'pp', 'xp', 'xp', 'xx', 'pp', 'pp', 'xp', 'pp', 'xx', '\\xa0p', 'xp', 'pp', 'xp', 'pp', 'xx', 'bp', 'pp', 'pp', 'bp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'xp', 'pp', 'pp', 'pp', 'pp', 'xp', 'xp', 'pp', '“p', 'xp', 'xp', 'pp', 'xp', 'pp', 'xp', 'xp']\n",
      "D_loss: 0.7516486644744873, G_loss: 0.7597109079360962\n",
      "32\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['pp', 'pp', 'pp', 'pp', '“p', 'pp', 'xp', 'Tp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', '\\xa0p', 'pp', 'xp', 'xp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', ' p', 'pp', ' p', 'pp', 'pp', 'xp', ' p', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'pp', 'pp', 'pp', 'pp', 'xp', 'xp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'xp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', '“p', 'xp', 'pp', 'pp', 'xp', 'pp', 'xp', 'pp']\n",
      "D_loss: 0.7589147686958313, G_loss: 0.7512335181236267\n",
      "36\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['pp', 'pp', 'pp', 'pp', '“p', 'pp', 'xp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', ' p', 'pp', ' p', 'pp', 'pp', 'pp', ' p', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', '“p', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp']\n",
      "D_loss: 0.7625831365585327, G_loss: 0.7474901676177979\n",
      "40\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', ' p', 'pp', ' p', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp']\n",
      "D_loss: 0.7581290602684021, G_loss: 0.751796543598175\n",
      "44\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', ' p', 'pp', ' p', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp']\n",
      "D_loss: 0.7572110891342163, G_loss: 0.7529639005661011\n",
      "48\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['pp', 'pp', 'pp', 'pp', 'Jp', 'pp', ' p', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Jp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', ' p', 'pp', ' p', 'pp', 'pp', 'pp', ' p', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Jp', 'pp', 'pp', 'Jx', 'pp', 'pp', 'pp', 'pp']\n",
      "D_loss: 0.7551816701889038, G_loss: 0.7549121975898743\n",
      "52\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['Jp', 'pp', 'pp', 'pp', 'Jp', 'pp', ' p', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Jæ', 'pp', 'xp', 'pp', 'pp', 'pp', 'fp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'pp', ' p', 'pp', ' p', 'pp', 'pp', 'pp', ' p', 'pp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'Jp', 'pp', 'Jp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Jp', 'xp', 'pp', 'Jx', 'pp', 'Jp', 'pp', 'pp']\n",
      "D_loss: 0.7556092143058777, G_loss: 0.754524827003479\n",
      "56\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['Jp', 'pp', 'pp', 'pp', 'Jp', 'pp', ' p', 'Zp', 'pp', 'xp', 'pp', 'pp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Jæ', 'pp', '\\xa0p', 'pp', 'pp', 'pp', 'fp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'pp', ' p', 'pp', ' p', 'pp', 'Jp', 'pp', ' p', 'pp', 'pp', 'Jp', 'Jp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'Jp', 'pp', 'Jp', 'xp', 'pp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Jp', 'xp', 'Op', 'Jx', 'pp', 'Jp', 'pp', 'pp']\n",
      "D_loss: 0.7492532134056091, G_loss: 0.761185884475708\n",
      "60\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'pp', 'pp', 'Jp', '…p', ' p', 'Zp', 'pp', 'xx', 'pp', 'pp', 'pp', 'Jp', 'pp', 'pp', 'pp', 'JJ', 'pp', 'xp', 'Ap', 'pp', 'Jp', 'Jæ', 'op', 'Jx', 'xp', 'pp', 'pp', 'fp', 'pp', 'JJ', 'Ap', 'xp', 'pp', 'pp', ' p', 'op', ' p', 'pp', 'Jp', 'xp', ' p', 'op', 'pp', 'JJ', 'Jp', 'xp', 'pp', 'bp', 'pp', 'Jp', 'pp', 'np', 'pp', 'pp', 'pp', 'pp', 'xp', 'xp', 'Ap', 'JJ', '…a', 'JJ', 'xp', 'Jp', 'pp', 'pp', 'pp', 'pp', 'xp', 'pp', 'pp', 'Jp', 'æp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'op', 'pp', 'Jp', 'Jp', 'pp', 'xp', 'pp', 'pp', 'Jp', 'xp', 'Op', 'Jx', 'pp', 'JJ', 'xp', 'pp']\n",
      "D_loss: 0.7544265985488892, G_loss: 0.755001962184906\n",
      "64\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'pp', 'pp', 'JJ', '…p', ' p', 'Zp', 'pp', 'xx', 'çp', 'pp', 'pp', 'Jæ', 'pp', 'pp', 'pp', 'JJ', 'pp', 'xp', 'Ap', 'pp', 'Jp', 'Jæ', 'op', 'Jx', 'xp', 'pp', 'pp', 'fp', 'pp', 'JJ', 'Ap', 'xp', 'pp', 'pp', ' p', 'op', ' p', 'pp', 'JJ', 'xp', ' p', 'op', 'pp', 'JJ', 'Jp', 'xp', 'pp', 'bp', 'pp', 'Jp', 'pp', 'np', 'pp', 'pp', 'pp', 'pp', 'xp', 'xx', 'Ap', 'JJ', 'Ja', 'JJ', 'xx', 'Jp', 'pp', 'pp', 'xp', 'pp', 'xx', 'pp', 'pp', 'Jæ', 'Zp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', 'op', 'pp', 'Jp', 'Jp', 'pp', 'xp', 'pp', 'pp', 'JJ', 'xp', 'Op', 'Jæ', 'xp', 'JJ', 'xp', 'pp']\n",
      "D_loss: 0.7554122805595398, G_loss: 0.7534781098365784\n",
      "68\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'pp', 'pp', 'JJ', '…p', '  ', 'Zp', 'pp', 'xx', ';p', 'pp', 'pp', 'JJ', 'pp', 'Up', 'Up', 'JJ', 'pp', 'xp', 'Ap', 'pp', 'Jp', 'JJ', \"'g\", 'Jx', 'xp', 'pp', 'pp', 'fx', 'pp', 'JJ', 'Ap', 'xp', 'pp', 'pp', '  ', \"'g\", '  ', 'pp', 'JJ', 'xp', ' p', \"'g\", 'pp', 'JJ', 'JJ', 'xp', 'pp', 'bp', 'pp', 'Jp', 'pp', 'np', 'pp', 'pp', 'pp', 'Jp', 'xp', 'xx', 'Ap', 'JJ', 'Ja', 'JJ', 'xx', 'Jp', '…p', 'pp', 'xp', 'pp', 'xx', 'pp', 'pp', 'JJ', 'Uæ', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'pp', 'np', \"'g\", 'pp', 'Jæ', 'Jæ', 'pp', 'xp', 'pp', 'pp', 'JJ', 'Lp', 'Op', 'Jæ', 'xp', 'JJ', 'xp', 'pp']\n",
      "D_loss: 0.7578589916229248, G_loss: 0.7502050995826721\n",
      "72\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'pæ', 'pp', 'JJ', '…p', '  ', 'Zæ', 'pp', 'xx', ';p', 'pp', 'pp', 'JJ', 'pp', 'Up', 'Up', 'JJ', 'pæ', 'Op', 'Ap', 'pp', 'Jp', 'JJ', \"'g\", 'Jx', 'Op', 'pp', 'pp', 'fx', 'pp', 'JJ', 'Ap', 'Op', 'cp', 'Jp', '  ', \"'g\", '  ', 'pp', 'JJ', 'Op', ' p', \"'g\", 'pp', 'JJ', 'JJ', 'Op', 'Kp', 'bg', 'pp', 'JJ', 'pp', 'Dn', 'pp', 'pp', 'pp', 'Jp', 'Op', 'xx', 'Ap', 'JJ', 'Ja', 'JJ', 'xx', 'Jp', 'Wp', 'pp', 'xx', 'pp', 'xx', 'Ap', 'pp', 'JJ', 'Ub', 'pp', 'pp', 'Æp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Dn', \"'g\", 'pp', 'Jæ', 'Jæ', 'pp', 'Op', 'pp', 'pp', 'JJ', 'Lp', 'Ox', 'JJ', 'xx', 'JJ', 'Op', 'pp']\n",
      "D_loss: 0.754324734210968, G_loss: 0.7532305121421814\n",
      "76\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'dæ', 'pp', 'JJ', '…g', '  ', 'Zæ', 'pp', 'xx', ';æ', 'pp', 'Up', 'JJ', 'pp', 'Up', 'Up', 'JJ', 'dæ', 'Ox', 'AA', 'pp', 'Jp', 'JJ', 'Dg', 'Jx', 'Ox', 'pp', 'pp', 'fx', 'pp', 'JJ', 'AA', 'Ox', 'bp', 'Jp', '  ', 'Dg', '  ', 'pp', 'JJ', 'Ox', ' p', 'Dg', 'pp', 'JJ', 'JJ', 'Ox', 'Kp', 'bb', 'Ap', 'JJ', 'pp', 'Dn', 'pp', 'pp', 'pp', 'Jx', 'Ox', 'xx', 'AA', 'JJ', 'Ja', 'JJ', 'xx', 'Jp', 'Wp', 'pp', 'xx', ';p', 'xx', 'Ap', 'Jp', 'JJ', 'Ub', 'pp', 'pp', 'Æp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Dn', 'Dg', 'pp', 'Jæ', 'Jæ', 'pp', 'Ox', 'pp', 'pp', 'JJ', 'Lx', 'Ox', 'JJ', 'xx', 'JJ', 'Ox', 'pp']\n",
      "D_loss: 0.757699191570282, G_loss: 0.748918890953064\n",
      "80\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'dæ', 'pp', 'JJ', '…g', '  ', 'Zb', 'pp', 'xx', ';;', 'pp', 'Up', 'JJ', 'pp', 'Up', 'Up', 'JJ', 'dæ', 'Ox', 'AA', 'pp', 'Jp', 'JJ', 'Dg', 'Jx', 'Ox', 'pp', 'pp', 'fx', 'pp', 'JJ', 'AA', 'Ox', 'bb', 'Jb', '  ', 'Dg', '  ', 'pp', 'JJ', 'Ox', ' p', 'Dg', 'Jp', 'JJ', 'JJ', 'Ox', 'Kp', 'bb', 'Ap', 'JJ', 'pp', 'Dn', 'pp', 'pp', 'pp', 'Jx', 'Ox', 'xx', 'AA', 'JJ', 'Ja', 'JJ', 'xx', 'Jp', 'Wp', 'pp', 'xx', ';p', 'xx', 'Ap', 'Jp', 'JJ', 'Ub', 'pp', 'pp', 'Æp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Dn', 'Dg', 'pp', 'Jæ', 'Jæ', 'pp', 'Ox', 'pp', 'pp', 'JJ', 'Lx', 'Ox', 'JJ', 'xx', 'JJ', 'Ox', 'pp']\n",
      "D_loss: 0.756432056427002, G_loss: 0.7495521306991577\n",
      "84\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'Jæ', 'pp', 'JJ', 'gg', '  ', 'Zb', 'pp', 'xx', ';;', 'pp', 'Up', 'JJ', 'pp', 'Up', 'Up', 'JJ', 'Jæ', 'Og', 'AA', 'pp', 'Jp', 'JJ', 'Dg', 'Jx', 'Og', 'pp', 'pp', 'Ux', 'pp', 'JJ', 'AA', 'Og', 'bb', 'Jb', '  ', 'Dg', '  ', 'pp', 'JJ', 'Og', ' p', 'Dg', 'Jp', 'JJ', 'JJ', 'Og', 'Kp', 'Jb', 'Ap', 'JJ', 'pp', 'Dn', 'pp', 'pp', 'pp', 'Jx', 'Og', 'xx', 'AA', 'JJ', 'Jb', 'JJ', 'xx', 'Jp', 'Wp', 'pp', 'xx', ';p', 'xx', 'Ap', 'Jg', 'JJ', 'Ub', 'pp', 'pp', 'Æp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Dn', 'Dg', 'pp', 'JJ', 'JJ', 'pp', 'Og', 'pp', 'pp', 'JJ', 'Lp', 'Ox', 'JJ', 'xx', 'JJ', 'Og', 'pp']\n",
      "D_loss: 0.7612963914871216, G_loss: 0.7431409955024719\n",
      "88\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'Jæ', 'pp', 'JJ', 'gg', '  ', 'æb', 'pp', 'xx', 'U;', 'pp', 'U;', 'JJ', 'pp', 'Up', 'Up', 'JJ', 'Jæ', 'Og', 'AA', 'pp', 'Jp', 'JJ', 'Dg', 'Jb', 'Og', 'pp', 'pp', 'UG', 'pp', 'JJ', 'AA', 'Og', 'æb', 'Jb', '  ', 'Dg', '  ', 'pp', 'JJ', 'Og', ' p', 'Dg', \"p'\", 'JJ', 'JJ', 'Og', 'Kp', 'Jb', 'Ap', 'JJ', 'pp', 'Db', 'pp', 'pp', ':g', 'Jx', 'Og', 'xx', 'AA', 'JJ', 'Jb', 'JJ', 'xx', 'Æg', 'Wp', 'p;', 'xx', ';;', 'xx', 'Ap', ':g', 'JJ', 'Ub', 'pp', 'pp', 'Æp', 'pp', 'pp', 'pp', 'pp', 'pp', 'Db', 'Dg', 'pp', 'JJ', 'JJ', 'pp', 'Og', 'pp', 'pp', 'JJ', 'Lp', 'Og', 'JJ', 'xx', 'JJ', 'Og', 'pp']\n",
      "D_loss: 0.7617140412330627, G_loss: 0.7413625121116638\n",
      "92\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', 'Jæ', 'pp', 'JJ', 'gg', '  ', 'æb', 'pp', 'xx', 'U;', 'Aæ', 'U;', 'JJ', 'Aæ', 'Up', 'Up', 'JJ', 'Jæ', 'Og', 'AA', 'pp', 'Jp', 'JJ', 'Dg', 'Jx', 'Og', 'Aæ', \"'p\", 'UG', 'pp', 'JJ', 'AA', 'Og', 'cb', 'Jb', '  ', 'Dg', '  ', 'pp', 'JJ', 'Og', '  ', 'Dg', \"p'\", 'JJ', 'JJ', 'Og', 'Kp', 'Jb', 'Ap', 'JJ', 'pp', 'Db', 'pp', 'Aæ', ':g', 'Jx', 'Og', 'xx', 'AA', 'JJ', 'Jb', 'JJ', 'xx', 'Æg', 'Wp', 'p;', 'xx', ';;', 'xx', 'Ap', ':g', 'JJ', 'Ub', 'pp', 'pp', 'Æp', 'pp', 'pp', 'pp', 'Kp', 'pp', 'Db', 'Dg', 'Ap', 'JJ', 'JJ', 'pp', 'Og', 'pp', 'pp', 'JJ', 'Lp', 'Dg', 'JJ', 'xx', 'JJ', 'Og', 'pp']\n",
      "D_loss: 0.756547212600708, G_loss: 0.7450369000434875\n",
      "96\n",
      "['I ', 'Ye', 'Ma', 'We', 'An', 'Th', 'Ca', 'Ma', 'Bu', 'Fi', 'Pr', 'He', 'Fr', 'I ', 'He', 'Fo', 'Fo', 'I ', 'Ma', 'Th', 'Go', 'Wh', 'He', 'I’', 'Th', 'Sa', 'Th', 'He', 'Ro', 'Na', 'Wh', 'I ', 'Go', 'Th', 'So', 'Me', 'Ca', 'Th', 'Ca', 'He', 'I ', 'Th', 'Co', 'Th', 'Gi', 'I ', 'Is', 'Th', 'Do', 'Sh', 'Go', 'I ', 'He', 'Ta', 'To', 'He', 'Oh', 'Ge', 'Th', 'Li', 'Go', 'In', 'Ma', 'I ', 'Le', 'O ', 'Ti', 'Bi', 'Le', 'Br', 'Le', 'Lo', 'Oh', 'I ', 'Pr', 'No', 'Wr', 'Ve', 'Be', 'Hu', 'Yo', 'Mo', 'Wh', 'Ta', 'Th', 'We', 'At', 'At', 'Be', 'Th', 'Wh', 'Yo', 'An', 'Th', 'Th', 'Kn', 'Le', 'I ', 'Th', 'We']\n",
      "['JJ', 'pp', \"''\", 'Ap', 'JJ', 'gg', '  ', 'fb', 'ip', 'xx', 'U;', 'Aæ', 'U;', 'JJ', 'Aæ', 'Up', 'Up', 'JJ', \"''\", 'xg', 'AA', 'pp', 'Jp', 'JJ', 'Dg', 'Jx', 'xg', 'Aæ', \"''\", 'UG', 'pp', 'JJ', 'AA', 'xg', 'cb', 'Jb', '  ', 'Dg', '  ', 'pp', 'JJ', 'xg', '  ', 'Dg', \"w'\", 'JJ', 'JJ', 'xg', 'Kp', 'Jb', 'Ap', 'JJ', 'pp', 'D;', 'pp', 'Aæ', ':g', 'Jx', 'xg', 'xx', 'AA', 'JJ', 'Jb', 'JJ', 'xx', 'Æg', 'Wp', 'p;', 'xx', ';;', 'xx', 'Ap', ':g', 'JJ', 'Ub', 'pp', 'pp', 'ÆÆ', 'pp', 'pp', 'pp', 'Kp', 'pp', 'D;', 'Dg', 'Ap', 'JJ', 'JJ', 'pp', 'xg', 'pp', 'pp', 'JJ', 'Lp', 'Dg', 'JJ', 'xx', 'JJ', 'xg', 'Ap']\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 20\n",
    "X = X_org[:,:max_length]\n",
    "batch_size = 100\n",
    "saved_model_path = '../models/lstm_ae_wa/lstm_ae_wa.ckpt'\n",
    "checkpoint_dir = '../models/lstm_ae_wa'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "losses = []\n",
    "step = 0\n",
    "curriculum_size = 2\n",
    "curriculum_error = 0.01\n",
    "with tf.Session() as sess:\n",
    "    #try:\n",
    "        #saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    #except:\n",
    "        #print('No model')\n",
    "    sess.run(init)\n",
    "        \n",
    "    # Train\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[b * batch_size:(b + 1) * batch_size]\n",
    "            batch_x = np.copy(batch_x)[:,:curriculum_size]\n",
    "            sess.run([D_train_step, G_train_step], feed_dict={x_input: batch_x})\n",
    "            \n",
    "            if i % 4 == 0 and b == 0:\n",
    "                D_loss_i, G_loss_i = sess.run([D_loss, G_loss], feed_dict={x_input: batch_x})\n",
    "                print(\"D_loss: {0}, G_loss: {1}\".format(D_loss_i, G_loss_i))\n",
    "                losses.append((D_loss_i, G_loss_i))\n",
    "                \n",
    "                x_gen_i = sess.run(decoder_output, feed_dict={x_input: batch_x})\n",
    "                x_gen_i = np.argmax(x_gen_i, axis=2)\n",
    "                print(i)\n",
    "                print(textify_samples(batch_x))\n",
    "                print(textify_samples(x_gen_i))\n",
    "                \n",
    "                if abs(D_loss_i - G_loss_i) < curriculum_error:\n",
    "                    #curriculum_size += 1\n",
    "                    pass\n",
    "\n",
    "                saver.save(sess, saved_model_path, global_step=step)\n",
    "            step += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZLy+v8BMDAMWTyA/BcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14c785c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "list_d_loss, list_g_loss = [], []\n",
    "for d_loss, g_loss in losses:\n",
    "    list_d_loss.append(d_loss)\n",
    "    list_g_loss.append(g_loss)\n",
    "\n",
    "plt.plot(list_d_loss)\n",
    "plt.plot(list_g_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
