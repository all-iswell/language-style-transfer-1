{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a mind to strike thee ere thou speak’st.\n",
      "Yet if thou say Antony lives, is well, Or friends wi\n",
      "I have half a mind to hit you before you speak again.\n",
      "But if Antony is alive, healthy, friendly with\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "import os\n",
    "\n",
    "original_dir_path = '../data/shakespeare/original/'\n",
    "modern_dir_path = '../data/shakespeare/modern/'\n",
    "\n",
    "def read_all_files(dir_path):\n",
    "    docs = \"\"\n",
    "    for filename in os.listdir(dir_path):\n",
    "        with open(dir_path + filename, 'r') as file:\n",
    "            docs += file.read()\n",
    "    return docs\n",
    "\n",
    "original_docs = read_all_files(original_dir_path)\n",
    "modern_docs = read_all_files(modern_dir_path)\n",
    "\n",
    "print(original_docs[:100])\n",
    "print(modern_docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'mind', 'to', 'strike', 'thee', 'ere', 'thou', 'speak’st', 'Yet', 'if', 'thou', 'say', 'Antony', 'lives,', 'is', 'well,', 'Or', 'friends', 'with', 'Caesar,', 'or', 'not', 'captive', 'to', 'him,', 'I’ll', 'set', 'thee', 'in', 'a', 'shower', 'of', 'gold', 'and', 'hail', 'Rich', 'pearls', 'upon', 'thee', 'Madam,', 'he’s', 'well', 'Well', 'said', 'And', 'friends', 'with', 'Caesar', 'Th’', 'art', 'an', 'honest', 'man', 'Caesar', 'and', 'he', 'are', 'greater', 'friends', 'than', 'ever', 'Make', 'thee', 'a', 'fortune', 'from', 'me', 'But', 'yet,', 'madam—', 'I', 'do', 'not', 'like', '“But', 'yet”', 'It', 'does', 'allay', 'The', 'good', 'precedence', 'Fie', 'upon', '“But', 'yet”', '“But', 'yet”', 'is', 'as', 'a', 'jailer', 'to', 'bring', 'forth', 'Some', 'monstrous', 'malefactor']\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode entire dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = (original_docs + \" \" + modern_docs).replace(\".\",\"\").replace(\"\\n\", \" \").split(\" \")\n",
    "print(dataset[:100])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(dataset)\n",
    "V = len(enc.classes_) #size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "original_sentences = original_docs.replace('.',\"\").split('\\n')\n",
    "modern_sentences = modern_docs.replace('.',\"\").split('\\n')\n",
    "\n",
    "#use smaller dataset\n",
    "original_sentences = original_sentences[:100]\n",
    "modern_sentences = modern_sentences[:100]\n",
    "\n",
    "max_length_org = max([len(s) for s in original_sentences])\n",
    "max_length_mod = max([len(s) for s in modern_sentences])\n",
    "max_length = max(max_length_org, max_length_mod)\n",
    "\n",
    "X_org = []\n",
    "for sentence in original_sentences:\n",
    "    words = sentence.split(\" \")\n",
    "    word_idx = np.array(enc.transform(words))\n",
    "    \n",
    "    arr = np.zeros(max_length)\n",
    "    arr[:len(words)] = word_idx\n",
    "    X_org.append(arr)\n",
    "    \n",
    "X_modern = []\n",
    "for sentence in modern_sentences:\n",
    "    words = sentence.split(\" \")\n",
    "    word_idx = np.array(enc.transform(words))\n",
    "    \n",
    "    arr = np.zeros(max_length)\n",
    "    arr[:len(words)] = word_idx\n",
    "    X_modern.append(arr)\n",
    "\n",
    "X_org = np.array(X_org)\n",
    "X_org = np.array(X_modern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overfit Autoencoder\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def dense(x, n1, n2, name):\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        weights = tf.get_variable(\"weights\", shape=[n1, n2], initializer=tf.random_normal_initializer(mean=0, stddev=0.01))\n",
    "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out\n",
    "\n",
    "input_dim = max_length\n",
    "n_l1 = 100\n",
    "n_l2 = 100\n",
    "z_dim = 2\n",
    "\n",
    "def encoder(x, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Encoder'):\n",
    "        e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n",
    "        e_dense_2 = tf.nn.relu(dense(e_dense_1, n_l1, n_l2, 'e_dense_2'))\n",
    "        latent_variable = dense(e_dense_2, n_l2, z_dim, 'e_latent_variable')\n",
    "        return latent_variable\n",
    "    \n",
    "def decoder(z, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_varaiable_scope().reuse_variables()\n",
    "    with tf.name_scope('Decodr'):\n",
    "        d_dense_1 = tf.nn.relu(dense(z, z_dim, n_l2, 'd_dense_1'))\n",
    "        d_dense_2 = tf.nn.relu(dense(d_dense_1, n_l2, n_l1, 'd_dense_2'))\n",
    "        output = dense(d_dense_2, n_l2, input_dim, 'd_output')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, [None, max_length])\n",
    "\n",
    "encoder_output = encoder(x_input)\n",
    "decoder_output = decoder(encoder_output)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(decoder_output - x_input))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10412233.0\n",
      "Loss: 4148458.5\n",
      "Loss: 4148045.25\n",
      "Loss: 4146912.75\n",
      "Loss: 4145893.5\n",
      "Loss: 4147863.5\n",
      "Loss: 4146901.0\n",
      "Loss: 4146691.0\n",
      "Loss: 4146446.25\n",
      "Loss: 4146140.25\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 10\n",
    "X = X_org\n",
    "batch_size = 100\n",
    "\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[:(b + 1) * batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x})\n",
    "            if i % 100 == 0:\n",
    "                batch_loss = sess.run(loss, feed_dict={x_input: batch_x})\n",
    "                print(\"Loss: {0}\".format(batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_encoder(x, lstm_units=2, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope('Encoder'):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units, initializer=initializer)\n",
    "\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_fw, x, dtype=tf.float32)\n",
    "        return state\n",
    "\n",
    "def lstm_decoder(x, z, lstm_units=2, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope('Decoder'):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units, initializer=initializer)\n",
    "        \n",
    "        zero_tensor = tf.zeros_like(x)\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_fw, zero_tensor, initial_state=z)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "beta1 = 0.9\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, [None, 30])\n",
    "embedding = tf.expand_dims(x_input, axis=2)\n",
    "\n",
    "encoder_output = lstm_encoder(embedding)\n",
    "decoder_output = lstm_decoder(embedding, encoder_output)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(decoder_output - embedding))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 86507032.0\n",
      "Loss: 86498856.0\n",
      "Loss: 86498848.0\n",
      "Loss: 86498848.0\n",
      "Loss: 86498848.0\n",
      "Loss: 86498848.0\n",
      "Loss: 86498848.0\n",
      "Loss: 86498848.0\n",
      "Loss: 86498848.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_epochs = 1000\n",
    "X = X_org[:,:30]\n",
    "batch_size = 100\n",
    "\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        for b in range(num_batches):\n",
    "            batch_x = X[:(b + 1) * batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x})\n",
    "            if i % 100 == 0:\n",
    "                batch_loss = sess.run(loss, feed_dict={x_input: batch_x})\n",
    "                print(\"Loss: {0}\".format(batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
